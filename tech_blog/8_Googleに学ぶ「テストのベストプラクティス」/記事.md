# Googleに学ぶ「テストのベストプラクティス」

## 参考文献
https://www.oreilly.co.jp/books/9784873119656/

## テスト概観

### テスト規模

Googleではテストを **小・中・大（＋巨大）** に分類する。
Googleが一番重視するのは **速度** と **決定性（同じ入力なら同じ結果）**。
小さいほど制約が強く、その分 **速くて安定しやすい**。
大きくなるほど柔軟だが **遅くなりやすく、非決定性（flaky）も入りやすい**。

- 小（ユニットテスト）：テストの主力
    - 単一プロセスで実行されなけばならない
- 中（統合テスト）：接続点を押さえる
    - 単一マシン内
- 大（E2Eテスト）：最小限で致命傷を防ぐ
    - 複数マシンもOK

![](https://storage.googleapis.com/zenn-user-upload/c5d6c801061a-20260104.png)
出典：https://testing.googleblog.com/2010/12/test-sizes.html

### テスト範囲

Googleでは、非常に大雑把なガイドラインとして、
ユニットテスト80%：ビジネスロジックの大部分を検証する
インテグレーションテスト15%：2つ以上のコンポーネント間の相互作用を検証する
エンドツーエンド（E2E）テスト5%：全システムを検証する
を目指している。

![](https://storage.googleapis.com/zenn-user-upload/afce5098a45c-20260103.png)
出典：Googleのソフトウェアエンジニアリング

## ユニットテスト

### 明確なテストを書く

#### メソッドではなく、挙動をテストせよ
テストを“メソッド単位”で作るのは最初は楽だが、長期的に破綻しやすい。

挙動とは「ある前提条件（given）のもとで、ある操作（when）をしたら、こうなる（then）」というシステムの保証。
ちなみに、テストは基本的に3ブロックで明示すると読みやすい：
前提（given）→ 行動（when）→ 検証（then）

メソッドと挙動の関係は多対多で、1メソッドが複数挙動を持つことも、複数メソッドの連携で1つの挙動が成立することもある。

「購入した物の表示がされる」
「残高が少ないと警告が出る」
みたいに挙動ごとにテストを分けると、各テストが短くなって意図が読みやすい。

#### テストにロジックを入れるな

**良いテストは、中身を一瞥しただけで「何を検証してるか」「期待結果が正しいか」が自明**であるべき。

**テストコード自体にロジック（演算子・条件分岐・ループ・文字列結合など）を入れると**、テストが何をしているか推論が必要になり、**テストの誤りや本番のバグを見逃しやすくなる**。
例：`baseUrl + "/albums"` のような**ちょっとした文字列結合**があるだけで、URLの `//` みたいなバグ（あるいは期待値側のミス）が**見えなくなる**。

**期待値は可能な限り“完成形”をベタ書き**し、説明的にする。そのための**多少の重複は安いコスト**（テストはDRYよりDAMPで考える）。



### テストとコード共有：DRYではなくDAMP

DAMP = *Descriptive And Meaningful Phrases*（説明的で意味がわかりやすい言い回し）

**テストコードではDRYのうまみが小さい**
良いテストは「なるべく変化しないように」ではなく、**対象の挙動が変わったら壊れてくれるのが望ましい**。だから変更容易性のためにDRYにする価値が本番コードほど高くない。
さらに、テストは自立して「正しさが自明」でないといけないので、**抽象化しすぎるとテスト自体がバグりやすくなる**。


#### 共有値

定数でテストデータを共有するとテストは短くなるが、
**なぜその値なのかが分かりにくくなり、定義まで探しに行く羽目になる**。
改善策は、**デフォルト値を持つヘルパー（/ビルダー）で生成**し、テストでは関心のある部分だけ上書きすること。

曖昧な名称を持つ共有値
```
private static final Account ACCOUNT_1 = Account.newBuilder()
    .setState(AccountState.OPEN).setBalance(50).build();

private static final Account ACCOUNT_2 = Account.newBuilder()
    .setState(AccountState.CLOSED).setBalance(0).build();

private static final Item ITEM = Item.newBuilder()
    .setName("Cheeseburger").setPrice(100).build();

// 何百行もの他のテスト

@Test
public void canBuyItem_returnsFalseWhenBalanceInsufficient() {
  assertThat(store.canBuyItem(ITEM, ACCOUNT_1)).isFalse();
}

@Test
public void canBuyItem_returnsFalseForClosedAccounts() {
  assertThat(store.canBuyItem(ITEM, ACCOUNT_2)).isFalse();
}
```

ヘルパーメソッドを用いた共有値
```
# コンストラクターの各パラメーターに任意のデフォルト値を定義することで
# コンストラクターをラップするヘルパーメソッド。
def newContact(
    firstName="Grace", lastName="Hopper", phoneNumber="555-123-4567"):
  return Contact(firstName, lastName, phoneNumber)

# 関心のあるパラメーターだけに値を指定してヘルパーメソッドを呼び出す。
def test_fullNameShouldCombineFirstAndLastNames(self):
  def contact = newContact(firstName="Ada", lastName="Lovelace")
  self.assertEqual(contact.fullName(), "Ada Lovelace")
```

#### 初期設定の共有

setUpで対象オブジェクトや協力オブジェクトを作るのは有効。

ただしテストがsetUp内の特定値（例: “Donald Knuth”）に依存すると、
**値の出どころが隠れて不完全なテストに見える**。

そういう場合はテスト内で値を明示し、必要ならsetUpのデフォルトを上書きする。

## テストダブル

まず **本物**、無理なら **フェイク**、それも無理なら（最小限の）**スタブ**、最後の逃げが（必要最小限の）**インタラクションテスト**。

### 基本概念

- **Fake**：簡易実装（例：インメモリDB）。動作は本物に近い
- **Stub**：固定の返り値を返す置き物
- **Mock**：呼び出し（相互作用）を検証するもの

### 優先順位

1. **本物の実装**
2. **Fake**
3. **Stub**
4. **Mock**

### 本物の実装
テストの第一選択は**本番と同じ実装**を使うこと。
そうするとテストが現実に近くなり、信頼性が上がる。
Googleではモッキングフレームワークの多用で「実装と同期が取れずリファクタ不能」みたいな汚染が多発し、**本物優先の流れ**になった。

#### 本物を使うかの判断軸
本物が **高速・決定的・依存が単純** なら本物を使うべき（例：値オブジェクト、日付、コレクション等）。

でも難しい場合はトレードオフ：

- **実行時間**：遅いならダブル検討。どこからが遅いかはテスト数と体感次第。
- **決定性**：外部サービスやマルチスレッド、時刻依存は非決定性の温床。必要ならダブルや密閉環境へ。
- **依存関係構築の重さ**：依存ツリーが巨大だと生成が辛い。ただし「だから全部モック」は短絡。理想はファクトリやDIで構築を共通化しつつ、必要なら差し替え可能にする。
  - 依存関係構築が重い例
    ```
    Foo foo = new Foo(new A(new B(new C()), new D()), new E(), ..., new Z());
    ```

### フェイキング

本物が現実的でないなら、たいてい**次善どころか最善**がフェイク。

フェイクは特に**API契約に忠実**であるべき。
ただし「テストが気にしない部分」（例：ハッシュ値そのもの、レイテンシ等）は完全一致不要な場合もある。
対応してない挙動は黙って誤魔化さず、**早く失敗（例外）**させるのが良い。

#### フェイクはテストされるべき
本物とフェイクの両方に同じテストを流して契約一致を確認する「契約テスト」がおすすめ。

### スタビング
特定の返り値やエラーを**簡単にシミュレート**できるのが強み。

#### 使いすぎの害

1. **テストが不明確**：スタブ設定がノイズになり、意図が読めない。
2. **テストが脆い**：実装詳細が漏れて、内部変更で壊れる。
3. **効果が落ちる**：本物の契約を複製するだけになり、正しさ保証が薄い。状態も保持できず検証が弱くなる。

典型例：大量スタブして最後に `verify()` するだけのテストは「動いたか」ではなく「呼んだか」しか見てない。

#### スタビングが適切なとき

* テスト対象を特定状態にするために「この値が必要」みたいに、**アサーションと直結する少数のスタブ**で済む場合。
* スタブが多いなら「使いすぎ」か「設計が複雑すぎ」のサイン。

##### 「アサーションと直結する少数のスタブ」とは

次のどれかを満たしてると「直結」寄り。

* **アサート対象の値を直接決める**
  例：`assertThat(count).isEqualTo(3)` をしたいから、依存から3件返させる
* **アサート対象の分岐を作る（成功/失敗・例外など）**
  例：`assertThrows(...)` させたいから、依存が例外を投げるようにする
* **テストしたい仕様に必須の“入力条件”を作る**
  例：認可が通る/通らない、課金が拒否される、みたいな条件を作る


### インタラクションテスト

#### 基本方針：ステートテスト優先

- **ステートテスト**：返り値や状態変化を検証（本当に動いたかを見る）。
- インタラクションテストは「呼んだ」しか分からず、正常動作の保証が弱い。
- 実装詳細にも依存しやすく、変更に過敏な「変更検知器テスト」になりがち。

#### インタラクションテストが正当化される場合

- 本物もフェイクも使えずステートテストできない（遅すぎる、フェイクがない等）。
- 呼び出し回数・順序が重要な性質（例：キャッシュでDBアクセスが増えてないか）。

インタラクションテストが必要な場合はテストが不十分のためより広範囲なテストを検討すべき。
例えば、インテグレーションテストの追加など

インタラクションテスト（呼び出し確認）だけではテストが不十分で、ステートテスト（結果や状態変化の確認）は置き換えられない。
だからユニットテストでステートテストができない場合は、より広範囲なテストで補うべきで、例えばDB呼び出しをユニットテストで verify するなら、別途「本物のDBで状態を検証する」インテグレーションテストを追加してリスクを減らすべき

#### ベストプラクティス

- **副作用のある“状態変更型”の呼び出しだけ**を検証する（sendEmail/save/logなど）。
- 副作用のない“非状態変更型”は原則不要（返り値で別のアサーションができるはず）。
- **過剰に指定しない**：関係ない引数や別の呼び出しまで縛ると無駄に壊れる。

## 大規模テスト

### 大テストの構造

大テストはだいたい次の4ステップで進む。

1. **テスト対象システム（SUT）を用意する**
2. **必要なテストデータをシード（初期状態）として入れる**
3. **SUTを動かしてトラフィック（操作・入力）を流す**
4. **挙動を検証する**

#### テスト対象システム

##### SUT（System Under Test）が大テストのスケールと性質を決める

ユニットテストは通常「同一プロセス内で、1クラス/モジュール中心」。
一方、大テストのSUTは**別プロセスや別マシンに分かれる**ことが多く、これが**遅さ**と**不安定さ**の主因になりやすい。

SUTは主に2軸で評価できる：

- **密閉性（Hermeticity）**：テスト外の要因（並行実行、インフラ不安定、他利用者など）からどれだけ隔離できてるか
- **忠実性（Fidelity）**：本番の構成・設定・トポロジーにどれだけ近いか

この2つはしばしば**トレードオフ**（本番に近づけるほど、外乱が増えて不安定になりがち）。

##### SUTの代表パターン

- **単一プロセスSUT**：全部を1バイナリに詰める。速くて隔離しやすいが、本番再現性は低い。
- **単一マシンSUT**：複数バイナリを1台で動かす（中テスト向き）。本番設定を使うと忠実性が上がる。
- **複数マシンSUT**：本番に近いが、ネットワークやマシン故障などで不安定になりやすく「大テスト」になる。
- **共有環境（ステージング/本番）**：既存環境を使うのでコストは低めだが、他利用と競合したり、デプロイ待ちが発生。特に本番はユーザー影響リスク。
- **ハイブリッド**：一部は自前で立てつつ、バックエンドなどは共有環境に依存。大規模組織では現実的にこれが必要になりがち。

##### “境界”でSUTを小さくするのがコツ（特にUI）

**UIテストは高コストで不安定**になりやすい理由：

- 見た目変更が多く、挙動が同じでもテストが壊れる
- 非同期が多く難しい

だから、バックエンドが公開APIを持つなら **UI/API境界で分割してAPI駆動のE2E寄り**にする方が保守しやすいことが多い。
```
### 例: E2Eをこう組む

1. `POST /public/signup` でユーザー作成（またはテスト用トークン取得）
2. `POST /public/cart/items` で商品追加
3. `POST /public/checkout` で注文作成
4. `GET /public/orders/{id}` で状態が `paid` / `confirmed` になったことを確認
5. （最後だけUIで）注文履歴画面を開いて注文が見えることを確認

**何が嬉しいか**

* UI操作が激減して速い・安定
* “テストデータ作成”がAPIで完結する
* 失敗時に原因がログ/レスポンスで追える（UIは原因が曖昧になりがち）
```

また **サードパーティ依存**は共有環境が無かったりコストがかかるので、本物APIを自動テストに使うのは非推奨で、ここも分割すべき重要な“シーム”。

##### 記録/再生で「大テストから小テストを作る」

サービス全体のテストダブル（モック/スタブ/フェイク）には「本物の契約に合ってる保証がない」問題がある。
その対策として外部では**消費者駆動契約テスト**（例：Pact、Spring Cloud Contract）がある。

Googleの人気アプローチは少し違い、

- **記録モード（大テスト）**で外部サービスへのトラフィックを記録
- **再生モード（小テスト）**でそれを再生して高速・安定に回す

という形で小テストを生成する。
ただし非決定性があるので、再生時は**リクエストとレスポンスのマッチング**が必要になり、性質としてはモックに近い。挙動が変わるとマッチしなくなるので、その場合は記録を取り直す必要があり、**記録テストを速く安定させるのが重要**になる。


#### 大テストのデータは2種類

- **シードデータ**：開始時点のSUT状態（初期データ）
- **テストトラフィック**：実行中にテストが送る入力

大規模で分離されたSUTだと、シードが特に大変になりがち（例：起動に必須のドメインデータ、現実的なベースライン、シード用APIの複雑さや直書きの副作用など）。

データの作り方は

- 手動作成
- 本番コピー
- **標本抽出（スマートサンプリング）**：量を減らしつつカバレッジを稼ぐ


#### 検証方法

* **手動**：回帰テストにも探索にも使えるが、規模が大きいほど人的コストが増えてスケールしない
* **アサーション**：ユニットテスト的に明示チェック（例：レスポンスに特定文字列が含まれる等）
* **A/B比較（差分テスト）**：2つのSUTに同じ入力を流して出力差分を見る。意図は明示されず、人間が差分確認する必要がある

### 大規模テストの類型

#### 1) 相互に作用し合う1つ以上のバイナリの機能テスト

* **SUT**: 単一マシンに閉じる or クラウドで分離デプロイ
* **データ**: 手動作成
* **検証**: アサーション
* ユニットテストでは再現できない「**実バイナリ間の相互作用**」（マイクロサービスなど）を、公開API経由で検証する。
* 複数バイナリになるほど複雑性が上がるのは当然の罰ゲーム。

#### 2) ブラウザーとデバイスのテスト

* 1. の特殊ケース（Web UI/モバイル）。
* エンドユーザーにとっては **公開API＝UIそのもの** なので、フロントエンド経由のテストがカバレッジ層を厚くする。

#### 3) パフォーマンス・負荷・ストレステスト

* **SUT**: クラウドで分離デプロイ
* **データ**: 手動 or 本番由来を多重化
* **検証**: 差分（パフォーマンス指標）
* 目的は **バージョン間の劣化検知** と **トラフィックスパイク耐性確認**。
* 難所:

  * スケールが上がるほど「バグを誘発する負荷」を作るのが難しい（システム全体の性質だから）。
  * **ノイズ（マシン性能/ネットワーク差）** が差分を汚染する。理想は同一マシンで両バージョンを比較。無理なら複数回実行して外れ値除去などで調整。

#### 4) デプロイ設定のテスト（設定のスモークテスト）

* **SUT**: 単一マシン密閉 or クラウド分離
* **データ**: なし
* **検証**: アサーション（クラッシュしない＝起動する）
* バグ原因はコードより **設定**（データファイル/DB/オプション）なことが多いので、起動時に設定を読んで正常起動するかを見る。

#### 5) 探索的テスト（手動）

* **SUT**: 本番 or 共有ステージング
* **データ**: 本番 or 既知のテスト領域
* **検証**: 手動
* 既知フロー反復ではなく、**新規シナリオ探索**で不審挙動・期待外れ・脆弱性を探す。
* 価値: 予期しない副作用の発見、到達パス増、見つけたバグを自動機能テストに落とし込める（手動ファズっぽい）。
* 制限: 手動はスケールしない。見つけた欠陥は **より頻繁に回せる自動テストで再現可能**にすべき。
* 代表手法: **バグバッシュ**（関係者が集団で手動テストしてバグを掘る会）。

#### 6) A/B差分（リグレッション）テスト

* **SUT**: クラウド上の分離された2環境（旧/新）
* **データ**: 本番 or サンプルを多重化
* **検証**: A/B差分比較
* 公開APIにトラフィックを送り、新旧の応答差を比較してリグレッションを検出。Googleで最も一般的になりがち（Hyrumの法則的に「公開API」は宣言ではなくユーザーが見える全部だから）。
* 変種:

  * **A/A**で非決定性やノイズ要因を洗い出す
  * **A/B/C**で「本番最新」「ベースライン」「変更込み」を並べ、累積影響も見やすくする
* 低コストで自動化しやすく、ローンチ済みシステムの副作用検知に強い。
* 制限（地味に重い現実）:

  * **承認**: 差分が良い悪いか人が理解して判断する必要が出やすい
  * **ノイズ**: 余計な差分ほど調査トイル増
  * **カバレッジ**: 有用なトラフィック生成が難しい
  * **構成**: SUT1つでも大変、SUT2つ同時は依存があると地獄が倍

#### 7) ユーザー受け入れテスト（UAT）

* **SUT**: 単一マシン密閉 or クラウド分離
* **データ**: 手動作成
* **検証**: アサーション
* 開発者のユニットテストは「意図通り」ではなく「意図通りに実装した」ことを確認しがちという弱点がある。
* UATは顧客（または代理）が想定する **ユーザージャーニー**の意図通り動作を、公開API経由で検証する自動テスト。
* “実行可能な仕様”系フレームワーク（Cucumber等）が使われることも。
* Googleでは仕様言語UATはそこまで多くない（作ってる人がコードも書けることが多いので必要性が低い）。

#### 8) プローバー（prober）とカナリア分析

* **SUT**: 本番
* **データ**: 本番
* **検証**: アサーション +（メトリクス）差分
* 本番監視に近いが構造は大規模テストっぽい。
* **プローバー**: 本番で決定的・読み取り中心の操作をして「最低限動く」を確認（本番のスモークテスト）。例: 検索して結果が返る、ただし中身は深く検証しない。
* **カナリア分析**: ロールアウト中にカナリア版とベースラインの健全性メトリクスを比較。
* 制限: ここで捕捉される問題は **すでにユーザー影響が出ている**。書き込み系だと本番状態を壊して非決定性・副作用を生むリスク。

#### 9) 障害復旧（DR）とカオスエンジニアリング

* **SUT**: 本番
* **データ**: 本番 + 手動作成（フォールトインジェクション）
* **検証**: 手動 +（メトリクス）差分
* 目的: システムが予期しない障害や変更にどれだけ耐えるか。
* Google例: 年1回の **DiRT**（大規模ウォーゲーム）。
* **カオスエンジニアリング**: バックグラウンドレベルで継続的に障害を注入し、回復も含めて強靭性を鍛える（GoogleはCatzillaで大量実施）。
* 制限: これも **検出時点でユーザー影響が起きうる**。DiRTはコストが非常に高く、組織的負荷も大きい。書き込み系フォールトはプローバー同様に副作用リスク。

#### 10) ユーザー評価

* **SUT**: 本番
* **データ**: 本番
* **検証**: 手動 or（メトリクス）差分
* 本番ベースでユーザー挙動データを大量収集し、将来機能の人気や問題を測る。UATの代替にもなる。
* 例:

  * **ドッグフーディング**（社内ユーザーに使わせる）
  * **実験（A/B）**（一部ユーザーに新挙動を出し、メトリクスで比較）
  * **評価者による評価**（「正解」がないML系などで重要。変更が良い/悪いの判断材料）
